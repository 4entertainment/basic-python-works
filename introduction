Hugging Face â€“ Using Transformers
Transformer modelleri genellikle oldukÃ§a bÃ¼yÃ¼ktÃ¼r. BirkaÃ§ milyon ila on milyarlarca parametreye sahip olan bu modellerin eÄŸitimi ve daÄŸÄ±tÄ±mÄ± karmaÅŸÄ±k bir Ã§abadÄ±r.

ğŸ¤— Transformers kÃ¼tÃ¼phanesi, bu sorunu Ã§Ã¶zmek iÃ§in oluÅŸturuldu. AmacÄ±, herhangi bir Transformer modelinin yÃ¼klenebilmesi, eÄŸitilebilmesi ve kaydedilebilmesi iÃ§in tek bir API saÄŸlamaktÄ±r. 

KÃ¼tÃ¼phanenin temel Ã¶zellikleri ÅŸunlardÄ±r:
KullanÄ±m KolaylÄ±ÄŸÄ±: En geliÅŸmiÅŸ NLP modelini indirme, yÃ¼kleme ve Ã§Ä±karsama iÃ§in sadece iki satÄ±r kod yazmak mÃ¼mkÃ¼ndÃ¼r. 
Esneklik: TÃ¼m modeller basit PyTorch nn.Module veya TensorFlow tf.keras.Model sÄ±nÄ±flarÄ±dÄ±r ve ilgili makine Ã¶ÄŸrenimi (ML) Ã§erÃ§evelerindeki diÄŸer modeller gibi iÅŸlenebilir. 
Basitlik: KÃ¼tÃ¼phane boyunca neredeyse hiÃ§bir soyutlama yapÄ±lmamÄ±ÅŸtÄ±r. "Tek dosyada her ÅŸey" temel bir kavramdÄ±r: bir modelin ileri geÃ§iÅŸi tamamen tek bir dosyada tanÄ±mlanmÄ±ÅŸtÄ±r, bÃ¶ylece kod kendisi anlaÅŸÄ±lÄ±r ve deÄŸiÅŸtirilebilir. Bu son Ã¶zellik, ğŸ¤— Transformers'Ä± diÄŸer ML kÃ¼tÃ¼phanelerinden oldukÃ§a farklÄ± kÄ±lar. Modeller, dosyalar arasÄ±nda paylaÅŸÄ±lan modÃ¼ller Ã¼zerine inÅŸa edilmemiÅŸtir; bunun yerine her bir model kendi katmanlara sahiptir. Modelleri daha yaklaÅŸÄ±labilir ve anlaÅŸÄ±lÄ±r hale getirmenin yanÄ± sÄ±ra, bu size bir model Ã¼zerinde deney yapmayÄ± diÄŸerlerini etkilemeden kolayca gerÃ§ekleÅŸtirme olanaÄŸÄ± saÄŸlar.



